# ollama-meter

âš¡ Apache Bench-style Ollama LLM performance benchmarking tool

## ğŸš€ Quick Start

```bash
# Install
cargo install --path .

# Basic usage - benchmark a single model
ollama-meter llama2:7b

# Compare multiple models
ollama-meter llama2:7b mistral:7b phi-2
```

## ğŸ“Š Features

- **Zero Dependencies** - Only requires Ollama to be installed
- **5-Minute Benchmark** - Get results fast with minimal setup  
- **Beautiful Output** - Progress bars and formatted tables
- **Multiple Formats** - Export as JSON, CSV, or Markdown
- **Cross-Platform** - Native support for Windows, macOS, and Linux
- **User-Friendly** - Clear error messages with helpful suggestions

## ğŸ“¦ Installation

### From Source

```bash
git clone https://github.com/yourusername/ollama-meter
cd ollama-meter
cargo build --release
# Binary will be at ./target/release/ollama-meter
```

### Prerequisites

- [Rust](https://rustup.rs/) (1.70.0 or later)
- [Ollama](https://ollama.ai/) running locally

## ğŸ¯ Usage

### Basic Benchmarking

```bash
# Benchmark a single model with default settings
ollama-meter llama2:7b

# Run 10 iterations instead of default 5
ollama-meter -n 10 llama2:7b

# Use a custom prompt
ollama-meter --prompt "Explain quantum computing" llama2:7b
```

### Comparing Models

```bash
# Compare multiple models
ollama-meter llama2:7b mistral:7b phi-2

# Export results to CSV
ollama-meter -e results.csv llama2:7b mistral:7b

# Output as JSON
ollama-meter -o json llama2:7b mistral:7b
```

### Advanced Options

```bash
# Full option list
ollama-meter --help

# Quiet mode (no progress bars)
ollama-meter -q llama2:7b

# Custom Ollama URL
ollama-meter --ollama-url http://remote:11434 llama2:7b

# Adjust generation parameters
ollama-meter -t 0.8 -m 200 llama2:7b  # temperature 0.8, max 200 tokens
```

## ğŸ“ˆ Output Example

```
âš¡ ollama-meter v0.1.0 - Benchmarking 2 models with 5 iterations each

Testing llama2:7b...
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% (5/5)

Testing mistral:7b...
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% (5/5)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model       â”‚ Avg Speed   â”‚ TTFT        â”‚ Success      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ llama2:7b   â”‚ 28.5 tok/s  â”‚ 234ms       â”‚ 100%         â”‚
â”‚ mistral:7b  â”‚ 31.2 tok/s  â”‚ 198ms       â”‚ 100%         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ† Winner: mistral:7b (9.5% faster, 15% lower TTFT)
ğŸ“Š Completed in 4m 13s
```

## ğŸ”§ Configuration

### Environment Variables

- `OLLAMA_HOST` - Override default Ollama URL (default: http://localhost:11434)

### Output Formats

- **table** (default) - Beautiful ASCII table
- **json** - Structured JSON output
- **csv** - Comma-separated values
- **markdown** - Markdown table format

## ğŸ—ï¸ Building from Source

```bash
# Development build
cargo build

# Optimized release build
cargo build --release

# Run tests
cargo test

# Run linter
cargo clippy
```

## ğŸ¯ Design Philosophy

ollama-meter follows the "Apache Bench" philosophy:

- **Simple** - One command to get started
- **Fast** - Minimal overhead, quick results
- **Clear** - Obvious what the results mean
- **Portable** - Works everywhere Ollama works

## ğŸ“Š Metrics Explained

- **Avg Speed** - Average tokens generated per second
- **TTFT** - Time To First Token (response latency)
- **Success Rate** - Percentage of successful completions

## ğŸ› Troubleshooting

### "Ollama is not running"
```bash
# Start Ollama
ollama serve
```

### "Model not found"
```bash
# Pull the model first
ollama pull llama2:7b
```

### Performance tips
- Close other applications using GPU
- Ensure adequate RAM for model size
- Use smaller models for faster benchmarks

## ğŸ“ License

MIT License

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ™ Acknowledgments

Built as a simpler alternative to existing enterprise-focused benchmarking tools, focusing on individual developer needs.